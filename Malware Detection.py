# -*- coding: utf-8 -*-
"""
Created on Wed Jun 21 17:46:19 2017

@author: Koo
"""
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D
from keras.layers import Dense, Dropout, Flatten
from sklearn.cross_validation import StratifiedKFold
import pandas as pd
import numpy as np
import os

from sklearn import svm #support vector machine
from sklearn.naive_bayes import GaussianNB
from sklearn import tree
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from sklearn.mixture import GaussianMixture, BayesianGaussianMixture
from sklearn.svm import libsvm

from sklearn.gaussian_process.kernels import RBF
from sklearn.gaussian_process import GaussianProcessClassifier

from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error

from matplotlib import pyplot as plt
from sklearn.datasets import make_classification
from sklearn.ensemble import ExtraTreesClassifier

def load_data(data):

    _data = data[:,0:-1]
    _labels= data[:,-1]
    return _data,_labels

    
def create_model():
    
    img_rows, img_cols= 5, 7 # 35, 1 / 5, 7
    input_shape = (img_rows, img_cols, 1) #35x1x1
    
    model = Sequential()
    model.add(Conv2D(32, kernel_size=(1, 1), #width, height
                     strides=(1,1),
                     activation='relu',
                     padding='same',
                     input_shape=input_shape))
    
    model.add(Conv2D(32, (1, 1), activation='relu', padding='same'))
    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))
  
    # fully connected layer
    model.add(Flatten()) 
    model.add(Dense(128, activation='relu')) # output dimension =128
    model.add(Dropout(0.5))
    model.add(Dense(2, activation='softmax'))
    
    model.compile(loss = 'mean_squared_error',  #binary_crossentropy, categorical_crossentropy
                      optimizer = 'adam', #adam, #adadelta
                      metrics=['accuracy'])

    return model

def train_and_evaluate_model(model, trainX, trainY, testX, testY):
    
    batch_size=128
    epochs=100
    
    model.fit(trainX, trainY,  
              validation_split = 0.2, 
              epochs = epochs, 
              verbose=1,
              validation_data=(testX, testY),  
              batch_size = batch_size)
    
    score = model.evaluate(testX, testY, verbose=0)
   
    return score[1]

def compare(count1, count2): #normal_data.shape[0], malware_data.shape[0]
    _min= count1 if count1<count2 else count2  
    return _min
    
def split_data(data, train_fector = 0.8):
    train_size = int(data.shape[0] * train_fector)
    train, test = data[0:train_size,:], data[train_size:data.shape[0],:]
    testX = test[:,0:-1]
    testY = test[:,-1]
    trainX = train[:,0:-1]
    trainY = train[:,-1]   
    return testX, testY, trainX, trainY

def decorder10_2stage(data):
    label = []
    for i in range(len(data)):
        if(data[i]==1):
            label.append([[0],[1]]);
        else:
            label.append([[1],[0]]);
    return label

def feature_normalization(data):
    feature_num = data.shape[1] 
    data_point = data.shape[0]  
    
    nomal_feature = np.zeros([data_point,feature_num])
    mu = np.zeros([1,feature_num]) 
    std = np.zeros([1,feature_num]) 
    
    mu=np.mean(data,0)
    mu=np.array(mu)
    mu=mu.reshape(1,feature_num)
    
    std=np.std(data,0)
    std=np.array(std)
    std=std.reshape(1,feature_num)

    for x in range(data_point):
        for y in range(feature_num):
            nomal_feature[x][y]=(data[x][y]-mu[0][y])/std[0][y]

    return nomal_feature, mu, std

def feature_normalization_test(data, mu, std):
    
    feature_num = data.shape[1] 
    data_point = data.shape[0]  
    
    nomal_feature = np.zeros([data_point,feature_num])
    
    for x in range(data_point):
        for y in range(feature_num):
            nomal_feature[x][y]=(data[x][y]-mu[0][y])/std[0][y]

    return nomal_feature
    
def acc(est,gnd):
    ## parameter
    total_num = len(gnd)
    acc = 0
    for i in range(total_num):
        if(est[i]==gnd[i]):
            acc = acc + 1
        else:
            acc = acc;
    return (acc / total_num)*100


def acc2(Gnd, Est):
    DataPoint = Gnd.shape[0]
    Gnd = Gnd.reshape([DataPoint,1])
    Est = Est.reshape([DataPoint,1])
    Tp = 0
    Fp = 0
    Tn = 0
    Fn = 0
    for i in range(DataPoint):
        if((Gnd[i] == 1) & (Est[i] == 1)):
            Tp += 1
        elif((Gnd[i] == 1) & (Est[i] == 0)):
            Fn += 1
        elif((Gnd[i] == 0) & (Est[i] == 1)):
            Fp += 1
        else:
            Tn += 1
            
    if(Tp==0):
        Precision = 0
        Recall = 0
    else:
        Precision = (Tp) / (Tp + Fp)
        Recall = (Tp) / (Tp + Fn)
    
    return Precision * 100, Recall * 100

def makescimodel():
    clf_svm = svm.SVC() # C-Support Vector Classification.
    clf_naive = GaussianNB() # 가우시안 나이브 베이지안 
    clf_DecisionTree = tree.DecisionTreeClassifier()
    clf_MLP = MLPClassifier(solver='lbfgs', alpha=1e-5,
                         hidden_layer_sizes=(5, 2), random_state=1)
    clf_RandomForest= RandomForestClassifier() #integer, optional (default=10) The number of trees in the forest.
    #clf_AdaBoost = AdaBoostClassifier() # (default=DecisionTreeClassifier)
    #clf_QuadraticDiscriminantAnalysis=QuadraticDiscriminantAnalysis()
    clf_GaussianMixture=GaussianMixture()
    clf_BayesianGaussianMixture= BayesianGaussianMixture()
    #clf_GaussianProcess=GaussianProcessClassifier(1.0 * RBF(1.0), warm_start=True)

    return [clf_svm, clf_naive, clf_DecisionTree, clf_MLP, clf_RandomForest, clf_GaussianMixture, clf_BayesianGaussianMixture]    

def fitscimodel(trainX, trainY, models):    
    for i , model in enumerate(models):
        print(str(i) + 'st model ing' )
        model.fit(trainX, trainY)
        print(str(i) + 'st model complete' )
    return models
    
if __name__ == "__main__":
    
    current_cwd = os.getcwd()
    dir_data = '\\data'
    
    file_name_malware = 'malware.csv'
    file_name_normal = 'normal.csv'
    feature_num = 41
    col_num = [i for i in range(5,feature_num)] # 35개의 featrue + 1개의 label = 36  
    
    os.chdir(current_cwd + dir_data)
    normal_frame = pd.read_csv(file_name_normal, usecols = (col_num))
    malware_frame = pd.read_csv(file_name_malware, usecols = (col_num))
    os.chdir(current_cwd)
    
    normal_data = normal_frame.values
    malware_data = malware_frame.values
    
    min_length= compare(normal_data.shape[0], malware_data.shape[0])
    
    normal_data = normal_data[:min_length]
    malware_data = malware_data[:min_length] 
    
    data = np.vstack([normal_data,malware_data])
    
    rand_perm_idx = np.random.permutation(data.shape[0])
    data = data[rand_perm_idx,:]
    
    del normal_frame, malware_frame, file_name_malware, file_name_normal
    del col_num, current_cwd, dir_data, feature_num, rand_perm_idx
    
    img_rows, img_cols= 5, 7
    
    n_folds=5
    data, labels = load_data(data)
    skf = StratifiedKFold(labels, n_folds=n_folds, shuffle=True)
    
    acc_list=[]

    labels[27158] = 1

#    data = data[0:100,:]
#    labels = labels[0:100]

    datapoint = data.shape[0]
    estcnn = np.zeros([datapoint])

    estsvm = np.zeros([datapoint])
    estnaive = np.zeros([datapoint])
    estDT = np.zeros([datapoint])
    estMLP = np.zeros([datapoint])
    estRF = np.zeros([datapoint])
    estAB = np.zeros([datapoint])
    estQD = np.zeros([datapoint])
    estGM = np.zeros([datapoint])
    estBG = np.zeros([datapoint])
    estGP = np.zeros([datapoint])    
    # k-fold cross-valication
    for i, (train, test) in enumerate(skf):
        
        print(train.shape)
        print(test.shape)
        print ("Running Fold", i+1, "/", n_folds)
        model = None # Clearing the NN.
        scimodels = None
        
        ## create model
        model = create_model()
        scimodels = makescimodel()
            
        trainX = data[train]
        trainY = labels[train]            
        testX = data[test]
        testY = labels[test]            
        
        ## normalization
        trainX, mu, std= feature_normalization(trainX) 
        testX = feature_normalization_test(testX, mu, std)
        
        ## cnn
        trainXcnn = trainX.reshape(trainX.shape[0], img_rows, img_cols, 1) # 35x1x1
        testXcnn = testX.reshape(testX.shape[0], img_rows, img_cols, 1)       
            
        trainYcnn = np.array(decorder10_2stage(trainY))
        trainYcnn = np.reshape(trainYcnn,[trainYcnn.shape[0],trainYcnn.shape[1]])       
        testYcnn = np.array(decorder10_2stage(testY))
        testYcnn = np.reshape(testYcnn,[testYcnn.shape[0],testYcnn.shape[1]]) 
        
        ## fit sci model 
        scimodels = fitscimodel(trainX, trainY, scimodels)
        
        #acc_list.append(train_and_evaluate_model(model, trainX, trainY, testX, testY))
        model.fit(trainXcnn, trainYcnn,  
                     epochs = 100, 
                     validation_data=(testXcnn, testYcnn))
        ## estimation
        estcnn[test] = model.predict(testXcnn).argmax(1)
        
        estsvm[test] = scimodels[0].predict(testX)
        estnaive[test] = scimodels[1].predict(testX)   
        estDT[test] = scimodels[2].predict(testX)
        estMLP[test] = scimodels[3].predict(testX)
        estRF[test] = scimodels[4].predict(testX)
#        estAB[test] = scimodels[5].predict(testX)
#        estQD[test] = scimodels[6].predict(testX)
        estGM[test] = scimodels[5].predict(testX)
        estBG[test] = scimodels[6].predict(testX)
#        estGP[test] = scimodels[9].predict(testX)   
        
accuracycnn = acc(estcnn, labels)
precisioncnn, recallcnn = acc2(labels, estcnn)

accuracysvm = acc(estsvm, labels)
precisionsvm, recallsvm = acc2(labels, estsvm)

accuracynaive = acc(estnaive, labels)
precisionnaive, recallnaive = acc2(labels, estnaive)

accuracyDT = acc(estDT, labels)
precisionDT, recallDT = acc2(labels, estDT)

accuracyMLP = acc(estMLP, labels)
precisionMLP, recallMLP = acc2(labels, estMLP)

accuracyRF = acc(estRF, labels)
precisionRF, recallRF = acc2(labels, estRF)

#accuracyAB = acc(estAB, labels)
#precisionAB, recallAB = acc2(labels, estAB)

#accuracyQD = acc(estQD, labels)
#precisionQD, recallQD = acc2(labels, estQD)

accuracyGM = acc(estGM, labels)
precisionGM, recallGM = acc2(labels, estGM)

accuracyBG = acc(estBG, labels)
precisionBG, recallBG = acc2(labels, estBG)

#accuracyGP = acc(estGP, labels)
#precisionGP, recallGP = acc2(labels, estGP)

accfactor = np.zeros([11,3])

accfactor[0,0], accfactor[0,1], accfactor[0,2] = accuracycnn, recallcnn, precisioncnn

accfactor[1,0], accfactor[1,1], accfactor[1,2] = accuracysvm, recallsvm, precisionsvm
accfactor[2,0], accfactor[2,1], accfactor[2,2] = accuracynaive, recallnaive, precisionnaive
accfactor[3,0], accfactor[3,1], accfactor[3,2] = accuracyDT, recallDT, precisionDT
accfactor[4,0], accfactor[4,1], accfactor[4,2] = accuracyMLP, recallMLP, precisionMLP
accfactor[5,0], accfactor[5,1], accfactor[5,2] = accuracyRF, recallRF, precisionRF
#accfactor[6,0], accfactor[6,1], accfactor[6,2] = accuracyAB, recallAB, precisionAB
#accfactor[7,0], accfactor[7,1], accfactor[7,2] = accuracyQD, recallQD, precisionQD
accfactor[8,0], accfactor[8,1], accfactor[8,2] = accuracyGM, recallGM, precisionGM
accfactor[9,0], accfactor[9,1], accfactor[9,2] = accuracyBG, recallBG, precisionBG
#accfactor[10,0], accfactor[10,1], accfactor[10,2] = accuracyGP, recallGP, precisionGP

model.summary()

print(accfactor)

## feature slection - information gain
forest = ExtraTreesClassifier(n_estimators=250,
                              random_state=0,criterion = 'entropy')
forest.fit(trainX, trainY)
importances = forest.feature_importances_
print(importances)
indices = np.argsort(importances)[::-1]
std = np.std([tree.feature_importances_ for tree in forest.estimators_],
             axis=0)

plt.figure()
plt.title("Feature importances",fontsize = 14)
plt.bar(range(trainX.shape[1]), importances[indices],
       color="r", yerr=std[indices], align="center")
plt.grid()
plt.xticks(range(trainX.shape[1]), indices)
plt.xlim([-1, trainX.shape[1]])
plt.ylabel('Entropy', fontsize =12)
plt.xlabel('Features',fontsize = 12)
plt.show()

plt.figure(0)
plt.plot(accfactor[0:6,0],'r-o')
plt.grid()
plt.xlabel('Classifiers', fontsize = 12)
plt.ylabel('Accuracy (%)',fontsize = 12)
plt.xticks(range(0,6),['CNN','SVM','Naive','DT','MLP','RF']) 

plt.figure(1)
plt.plot(accfactor[0:6,1],'r-o')
plt.grid()
plt.xlabel('Classifiers', fontsize = 12)
plt.ylabel('Recall (%)',fontsize = 12)
plt.xticks(range(0,6),['CNN','SVM','Naive','DT','MLP','RF']) 

plt.figure(2)
plt.plot(accfactor[0:6,2],'r-o')
plt.grid()
plt.xlabel('Classifiers', fontsize = 12)
plt.ylabel('Precision (%)',fontsize = 12)
plt.xticks(range(0,6),['CNN','SVM','Naive','DT','MLP','RF']) 


accfactor2 = accfactor[0:6,:]
